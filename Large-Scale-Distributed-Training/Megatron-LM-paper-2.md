# Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

- paper: https://arxiv.org/pdf/2104.04473.pdf
- github: https://github.com/NVIDIA/Megatron-LM
- æ•´ä½“æ¥è¯´ï¼Œè¿™ç¯‡æ–‡ç« æ›´è´´è¿‘å®é™…åœºæ™¯ï¼Œå°±å¦‚ä½•æ··ç”¨å„ç§æŠ€å·§ï¼Œä½¿è¿è¡Œæ•ˆç‡æœ€å¤§åŒ–å±•å¼€äº†ç ”è®¨

## Abstract
1. In this paper, we show how tensor, pipeline, and data parallelism can be composed to scale to thousands of GPUs. We propose a novel interleaved pipelining schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches.
2. åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†å¼ é‡ã€pipelineå’Œæ•°æ®å¹¶è¡Œåº¦æ‰©å±•åˆ°æ•°åƒä¸ªgpuã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„äº¤é”™æµæ°´çº¿è°ƒåº¦ï¼Œä¸ç°æœ‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œå…¶å†…å­˜å ç”¨å¯ä»¥æé«˜10+%çš„ååé‡ã€‚

## INTRODUCTION
1. In addition, we studied the interaction between the various components affecting throughput, both empirically and analytically when possible. Based on these studies, we offer the following guiding principles on how to configure distributed training:
    1. æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†å½±å“ååé‡çš„å„ç§æˆåˆ†ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¹¶åœ¨å¯èƒ½çš„æƒ…å†µä¸‹è¿›è¡Œäº†ç»éªŒå’Œåˆ†æã€‚åŸºäºè¿™äº›ç ”ç©¶ï¼Œæˆ‘ä»¬æä¾›äº†ä»¥ä¸‹å…³äºå¦‚ä½•é…ç½®åˆ†å¸ƒå¼åŸ¹è®­çš„æŒ‡å¯¼åŸåˆ™ï¼š
2.  Different forms of parallelism interact in non-trivial ways: the parallelization strategy has an impact on the amount of communication, the compute efficiency with which kernels are executed, as well as the idle time workers spend waiting for computation due to pipeline flushes (pipeline bubbles). For example, in our experiments, we found that sub-optimal combinations of tensor and pipeline model parallelism can lead to up to 2Ã— lower throughput, even with high-bandwidth network links between servers; tensor model parallelism is effective within a multi-GPU server, but pipeline model parallelism must be used for larger models.
    1.  ä¸åŒå½¢å¼çš„å¹¶è¡Œæ€§ä»¥éå¹³å‡¡çš„æ–¹å¼ç›¸äº’ä½œç”¨ï¼šå¹¶è¡ŒåŒ–ç­–ç•¥å¯¹é€šä¿¡é‡ã€æ‰§è¡Œå†…æ ¸çš„è®¡ç®—æ•ˆç‡ä»¥åŠç”±äºpipelineåˆ·æ–°ï¼ˆpipelineæ°”æ³¡ï¼‰è€Œç­‰å¾…è®¡ç®—çš„ç©ºé—²æ—¶é—´éƒ½æœ‰å½±å“ã€‚ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°å¼ tensorå’Œpipelineæ¨¡å‹å¹¶è¡Œæ€§çš„æ¬¡ä¼˜ç»„åˆå¯ä»¥å¯¼è‡´é«˜è¾¾2Ã—çš„ä½ååé‡ï¼Œå³ä½¿æ˜¯æœåŠ¡å™¨ä¹‹é—´çš„é«˜å¸¦å®½ç½‘ç»œé“¾æ¥ï¼›tensoræ¨¡å‹å¹¶è¡Œæ€§åœ¨å¤šgpuæœåŠ¡å™¨ä¸­æ˜¯æ›´é«˜æ•ˆçš„ï¼Œä½†æ˜¯pipelineæ¨¡å‹å¹¶è¡Œæ€§å¿…é¡»ç”¨äºæ›´å¤§çš„æ¨¡å‹ã€‚
    2.  ä¸»è¦æ˜¯è¯´pipelineå¹¶è¡Œä¸­çš„ç©ºæ¡£/æ°´æ³¡(bubbles)å¯èƒ½ä¼šé€ æˆ2xä»¥ä¸Šçš„ååé‡ä¸‹é™ï¼Œè™½ç„¶tensor-model-parallelé«˜æ•ˆï¼Œä½†æ˜¯åœ¨è¶…å¤§æ¨¡å‹ä¸­ï¼Œpipelineå¹¶è¡Œä¹Ÿæ˜¯ä¸å¯æˆ–ç¼ºçš„
3.  The schedule used for pipeline parallelism has an impact on the amount of communication, the pipeline bubble size, and memory used to store activations. We propose a novel interleaved schedule that can improve throughput by as much as 10% compared to previously-proposed schedules [20, 30] with comparable memory footprint.
    1.  ç”¨äºpipelineå¹¶è¡Œæ€§çš„è®¡åˆ’å¯¹é€šä¿¡é‡ã€pipelineæ°”æ³¡å¤§å°å’Œç”¨äºå­˜å‚¨æ¿€æ´»çš„å†…å­˜éƒ½æœ‰å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„äº¤é”™è°ƒåº¦ï¼Œä¸ä¹‹å‰æå‡ºçš„å…·æœ‰ç±»ä¼¼å†…å­˜å ç”¨çš„è°ƒåº¦[20,30]ç›¸æ¯”ï¼Œå®ƒå¯ä»¥æé«˜é«˜è¾¾10%çš„ååé‡ã€‚
    2.  è¿™ä¸ªscheduleæˆ‘ä»¬å¯ä»¥åœ¨åæ–‡åŠä»£ç ä¸­çœ‹åˆ°ï¼Œçœ‹èµ·æ¥æœ‰ç‚¹ç±»ä¼¼è·‘å¤šæ­¥stepä¸€æ¬¡çš„æ„Ÿè§‰
4.  Values of hyperparameters such as microbatch size have an impact on the memory footprint, the arithmetic efficiency of kernels executed on the worker, and the pipeline bubble size. In our experiments, the optimal value of the microbatch size is problem-dependent and can increase throughput by 15%.
    1.  è¶…å‚æ•°çš„å€¼ï¼Œå¦‚microbatchå¤§å°ï¼Œä¼šå¯¹å†…å­˜å ç”¨ã€åœ¨å·¥ä½œè€…ä¸Šæ‰§è¡Œçš„å†…æ ¸çš„ç®—æœ¯æ•ˆç‡å’Œpipelineæ°”æ³¡å¤§å°äº§ç”Ÿå½±å“ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œmicrobatchå¤§å°çš„æœ€ä¼˜å€¼æ˜¯ä¸é—®é¢˜ç›¸å…³çš„ï¼Œå¯ä»¥å¢åŠ 15%çš„ååé‡ã€‚
    2.  è¿™ä¸ªè™½ç„¶å¡èµ·æ¥æ„Ÿè§‰æœ‰ç‚¹ç‰µå¼ºï¼Œå®é™…ä¸Šåœ¨pipelineå¹¶è¡Œçš„æ—¶å€™ï¼Œmicrobatchå¤§å°å†³å®šäº†pipelineå„èŠ‚ç‚¹çš„ç­‰å¾…æ—¶é•¿ï¼Œè¿™å¯¹äºpipelineçš„å¹¶è¡Œæ•ˆç‡å°¤ä¸ºå…³é”®
5.  At scale, distributed training is communication-intensive. When training a trillion-parameter model on 3072 GPUs, our implementation used an effective bisection bandwidth of 892 GB/s for pipeline-parallel communication, and 13 TB/s for data-parallel communication. Using slower inter-node interconnects or more communication-intensive partitionings would hinder scaling performance.
    1.  åœ¨å¤§è§„æ¨¡ä¸Šï¼Œåˆ†å¸ƒå¼åŸ¹è®­æ˜¯æ²Ÿé€šå¯†é›†å‹çš„ã€‚å½“åœ¨3072ä¸ªgpuä¸Šè®­ç»ƒä¸€ä¸ªä¸‡äº¿å‚æ•°æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬çš„å®ç°ä½¿ç”¨äº†892GB/sçš„æœ‰æ•ˆäºŒåˆ†å¸¦å®½ç”¨äºpipelineå¹¶è¡Œé€šä¿¡ï¼Œ13TB/sç”¨äºæ•°æ®å¹¶è¡Œé€šä¿¡ã€‚ä½¿ç”¨è¾ƒæ…¢çš„èŠ‚ç‚¹é—´äº’è¿æˆ–æ›´å¤šçš„é€šä¿¡å¯†é›†å‹åˆ†åŒºå°†é˜»ç¢æ‰©å±•æ€§èƒ½ã€‚
    2.  è¿™ä¸€ç‚¹å®é™…ä¸Šæ˜¯æœ‰æŒ‡å¯¼æ„ä¹‰çš„ï¼Œå¤§æ¨¡å‹è®­ç»ƒåšåˆ°æœ€åï¼Œä¸»è¦çš„ä¼˜åŒ–ä¸€å®šæ˜¯åœ¨ä¼˜åŒ–é€šä¿¡æ—¶é—´æ¶ˆè€—ä¸Šï¼Œå°¤å…¶æ˜¯pipelineå¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œ

## MODES OF PARALLELISM
- åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¹¶è¡Œæ€§æŠ€æœ¯ï¼Œä»¥ä¿ƒè¿›å¯¹ä¸é€‚åˆå•ä¸ªGPUå†…å­˜çš„å¤§å‹æ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†pipelineæ¨¡å‹çš„å¹¶è¡Œæ€§å’Œå¼ é‡æ¨¡å‹çš„å¹¶è¡Œæ€§ï¼ˆå¦‚å›¾2æ‰€ç¤ºçš„ç»„åˆï¼‰ä¸æ•°æ®çš„å¹¶è¡Œæ€§ç»“åˆèµ·æ¥ã€‚æˆ‘ä»¬å°†æ­¤ç®€ç§°ç§°ä¹‹ä¸ºPTD-Pã€‚
- ![](./imgs/p2-f1.jpg)
1. Data Parallelism
   1. DP/DDPå¤§å®¶éƒ½å¾ˆç†Ÿæ‚‰ï¼Œå°±ä¸å†èµ˜è¿°äº†
2. Pipeline Model Parallelism
   1. pipelineå¹¶è¡Œæ˜¯æŠŠä¸€ä¸ªæ¨¡å‹çš„ä¸åŒlayeræ‹†åˆ†åˆ°ä¸åŒçš„å¡ä¸Šä»è€Œå®ç°çš„ä¸€ç§å¹¶è¡Œæ–¹å¼ï¼Œ
   2. åœ¨æ¯æ‰¹å¼€å§‹å’Œç»“æŸæ—¶ï¼Œè®¾å¤‡éƒ½ç©ºé—²ã€‚æˆ‘ä»¬æŠŠè¿™ç§ç©ºé—²æ—¶é—´ç§°ä¸ºpipeline bubbleï¼Œå¹¶å¸Œæœ›ä½¿å®ƒå°½å¯èƒ½å°ã€‚
   3. å¼‚æ­¥å’Œè¾¹ç•Œæ€çš„æ–¹æ³•å¦‚ PipeMare, PipeDream, and PipeDream-2BWå¯ä»¥è®©pipelineå®Œå…¨æµèµ·æ¥ï¼Œä½†æ˜¯å†è¯­ä¹‰ä¸Šå´æ²¡æœ‰åšåˆ°å¾ˆå¥½çš„æŠŠæ§å¯¹é½ã€‚æœ¬æ–‡ä¸è®¨è®ºè¿™äº›æ–¹æ³•ã€‚
   4. GPipe
   5. ![](./imgs/p2-f2.jpg)
   6. è¿™ä¸ªå…¶å®æœ‰ç‚¹ç±»ä¼¼è·‘å¤šä¸ªstepç´¯åŠ gardï¼Œç„¶åä¸€æ¬¡åˆ°åŒæ­¥ç‚¹æ—¶(è¿™é‡Œæ˜¯8)åšä¸€æ¬¡optim.step()æ¥å®Œæˆæƒé‡çš„æ›´æ–°åŒæ­¥ã€‚è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå›¾å‡è®¾äº†pipelineæ˜¯4ï¼Œ8ä¸ªå‘¨æœŸæ¨ªè½´é•¿åº¦ä¸º8(fp)+16(bp)+9(bubble)=33
   7. ![](./imgs/p2-f3.jpg)
   8. æˆ‘å¯¹æ¯”åšäº†ä¸ªå¦‚æœå¸¸è§„è·‘çš„å›¾ï¼Œ1ä¸ªå‘¨æœŸæ¨ªè½´é•¿åº¦ä¸º4(fp)+8(bp)=12ï¼Œ8ä¸ªå‘¨æœŸä¸º12*8=96ï¼Œå¯¹æ¯”å¯ä»¥çœ‹å‡ºï¼Œå°¤å…¶æ˜¯åœ¨pipelineå¹¶è¡Œçš„åœºæ™¯ï¼Œè·‘å¤šbatchåç´¯è®¡æ¢¯åº¦æ›´æ–°ä¸€æ¬¡æ˜¯éå¸¸é«˜æ•ˆæœ‰æ„ä¹‰çš„
   9. Schedule with Interleaved Stages
   10. è¿™å°±æ›´è¿›ä¸€æ­¥äº†ï¼Œå‡è®¾äº†æ¯å¼ å¡å†…éƒ¨è¿˜å¯ä»¥æŠŠæµæ°´å¼€èµ·æ¥ï¼Œä¾‹å¦‚åŸæ¥è®¾å¤‡1æœ‰å±‚1âˆ’4ï¼Œè®¾å¤‡2æœ‰å±‚5âˆ’8ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è®©è®¾å¤‡1æœ‰å±‚1,2,9,10,è®¾å¤‡2æœ‰å±‚3,4,11,12,è¿™æ ·å°±å¯ä»¥è¿›ä¸€æ­¥å‹ç¼©bubble
   11. ![](./imgs/p2-f4.jpg)
   12. è¿™é‡Œæ·±è“è‰²è¡¨ç¤ºç¬¬ä¸€ä¸ªchunk(i.e 0,1)ï¼Œæµ…è“è‰²è¡¨ç¤ºç¬¬äºŒä¸ªchunk(i.e 9,10)ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç†æƒ³çŠ¶æ€ä¸‹ï¼Œå¹¶è¡Œæ•ˆç‡è¢«è¿›ä¸€æ­¥æé«˜ã€‚å®é™…ä¸Šä½œè€…è¿™é‡Œä¹Ÿæ„è¯†åˆ°ï¼Œè¿™æ ·åšä¼šå¸¦æ¥é¢å¤–çš„é€šä¿¡é‡ï¼Œåœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œä½œè€…ä¼šè®¨è®ºå¦‚ä½•åœ¨å¤šgpuæœåŠ¡å™¨(ä¾‹å¦‚ï¼ŒDGXA100èŠ‚ç‚¹)ä¸­åˆ©ç”¨8ä¸ªæ— é™å¸¦ç½‘å¡æ¥å‡å°‘è¿™ç§é¢å¤–é€šä¿¡çš„å½±å“ã€‚
3. Tensor Model Parallelism
   1. è¿™é‡Œå°±æ˜¯ä¸Šç¯‡æ–‡ç« çš„å†…å®¹ï¼Œä¸å†èµ˜è¿°
   
## PERFORMANCE ANALYSIS OF PARALLELIZATION CONFIGURATIONS

- è¿™éƒ¨åˆ†ä½œè€…ä¸€é€šåˆ†æï¼Œå¾—åˆ°äº†ä¸€äº›è¦ç‚¹ï¼Œè¿™è¾¹çš„å°±ç›´æ¥è´´è¦ç‚¹äº†ã€‚è¿™äº›è¦ç‚¹ä¹Ÿæ˜¯æ¯”è¾ƒç¬¦åˆç›´è§‰çš„

1. Tensor and Pipeline Model Parallelismçš„è¦ç‚¹
   1. å½“è€ƒè™‘ä¸åŒå½¢å¼çš„æ¨¡å‹å¹¶è¡Œæ€§æ—¶ï¼Œå½“ä½¿ç”¨ğ‘”-gpuæœåŠ¡å™¨æ—¶ï¼Œtensoræ¨¡å‹å¹¶è¡Œåº¦é€šå¸¸åº”è¯¥ä½¿ç”¨åˆ°ğ‘”åº¦ï¼Œç„¶åpipelineæ¨¡å‹å¹¶è¡Œæ€§å¯ä»¥ç”¨æ¥è·¨æœåŠ¡å™¨æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹ã€‚
   2. ç®€å•è¯´å°±æ˜¯tensoræ¨¡å‹å¹¶è¡Œå°½å¯èƒ½åœ¨å•ä¸ªæœåŠ¡å™¨å†…éƒ¨coverï¼Œgè¡¨ç¤ºè¯¥æœåŠ¡å™¨æœ‰å‡ å¼ å¡ï¼Œé€šå¸¸ä¸º8ï¼Œpipelineæ¨¡å‹å¹¶è¡Œæ€§å¯ä»¥ç”¨äºè·¨æœåŠ¡å™¨çš„å¹¶è¡Œ
2. Data and Model Parallelismçš„è¦ç‚¹
   1. Pipeline Model Parallelismä¸­microbatcheså¸¦æ¥çš„å½±å“
   2. ![](./imgs/p2-f5.jpg)
   3. 





# Welcome to Bei's Blog

EZ website for sharing blogs with markdown~

## Large-Scale Distributed Training

### Megatron-LM

#### paper & walk-in

1. [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf)
    - [walk-in](./Large-Scale-Distributed-Training/Megatron-LM-paper-1.md)
2. [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf)
    - [walk-in](./Large-Scale-Distributed-Training/Megatron-LM-paper-2.md)
3. [](https://arxiv.org/pdf/2205.05198)
    - [walk-in](./Large-Scale-Distributed-Training/Megatron-LM-paper-3.md)

#### code
1. [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
   - [walk-in](./Large-Scale-Distributed-Training/Megatron-LM-code-1.md)





